# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1abQa3W71ul_NvzfQCXDYMd6sNEzFaT2O

# A Comparison of Time Series Methods to Predict COVID-19 Cases
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from math import sqrt

from google.colab import drive
drive.mount('/content/drive')

"""### Data Collection"""

covid = pd.read_json('/content/drive/MyDrive/Colab Notebooks/world-covid-data.json')
covid = covid.T
covid = covid.drop(["OWID_WRL"])
most_populated = covid.sort_values("population", ascending=False).head(10)

"""### Finding Correlation between attributes"""

data = pd.DataFrame.from_records(covid['data'])
data = data.T
data.columns = covid.index

def getAvgNewCases(data):
  count = 0
  newCases = 0
  for i in range(len(data)):
    try:
      # print(data.iloc[i]['new_cases'])
      newCases += data.iloc[i]['new_cases']
      count += 1
    except:
      # print("nope")
      pass
  if count != 0:
    return newCases / count
  return np.nan

avgNewCases = data.aggregate(getAvgNewCases, axis=0).to_frame()
avgNewCases.columns = ['avg_new_cases']

merged = pd.merge(covid, avgNewCases, how='left', left_index=True, right_index=True)
merged = merged.drop(columns=['continent', 'location', 'data'])

#fill nan with average of columns
for column in merged.columns:
  merged[column].fillna((merged[column].mean()), inplace=True)

merged['avg cases per capita'] = merged['avg_new_cases'] / merged['population']
merged.drop(merged.tail(1).index,inplace=True) # drop last n rows
merged

# getting corelation for all
correlation = merged.corr().loc['avg_new_cases',:]
correlation = correlation.to_frame()
correlation.drop(index=['avg_new_cases', 'avg cases per capita'], inplace=True)
import seaborn as sn
import matplotlib.pyplot as plt
sn.heatmap(correlation.sort_values("avg_new_cases", ascending=False), annot=True)
plt.show()

"""#### Evaluation

From the above graph, we concluded that none of the attributes correlate to the number of average case per day of the countries.

### Predicting new cases per day
"""

data = pd.DataFrame.from_records(most_populated['data'])
data = data.T
data.columns = most_populated.index

def getNewCases(data):
  new_cases = []
  for i in range(len(data)):
    try:
      new_case = data[i]["new_cases"]
      if new_case == 0 and len(new_cases) > 0:
        new_cases.append(new_case)
      elif new_case > 0:
        new_cases.append(new_case)
    except:
      if len(new_cases) > 0:
        new_cases.append(0)
      pass
  return pd.Series(new_cases)

new_cases = most_populated["data"].apply(getNewCases)
new_cases

"""#### Linear Regression (Polynomial)"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
def applyPoly(data):
  split = int(len(data.dropna())*0.94)
  train = data.dropna()[:split]
  test = data.dropna()[split:]
  p = make_pipeline(PolynomialFeatures(5),LinearRegression())  
  p.fit(np.arange(train.index[0], train.index[-1]+1).reshape(-1,1), train)
  output = p.predict(np.arange(test.index[0], test.index[-1]+1).reshape(-1,1))
  polyRMSE.append(sqrt(mean_squared_error(test, output)))
  return pd.Series(output, index=test.index)

polyRMSE = list()
predictedPoly = new_cases.apply(applyPoly, axis=1)

"""#### Auto Regressive Integrated Moving Average (ARIMA)"""

from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.arima_model import ARMA
def applyARMA(data):
  split = int(len(data.dropna())*0.94)
  train = data.dropna()[:split]
  test = data.dropna()[split:]
  history = [x for x in train]
  predictions = list()
  for t in range(len(test)):
    try:
      model = ARIMA(history, order=(5,1,0))
      model_fit = model.fit(disp=0)
      output = model_fit.forecast()
      yhat = output[0]
      predictions.append(yhat)
      obs = test[t+test.index[0]]
      history.append(obs)
    except:
      arimaRMSE.append(np.nan)
      return np.nan
  arimaRMSE.append(sqrt(mean_squared_error(test, predictions)))
  return pd.Series(predictions, index=test.index)

arimaRMSE = list()
predictedARIMA = new_cases.apply(applyARMA, axis=1)

"""#### Bayesian Ridge Regression (BRR)"""

from sklearn.linear_model import BayesianRidge, LinearRegression
def applyBRR(data):
  split = int(len(data.dropna())*0.94)
  train = data.dropna()[:split]
  test = data.dropna()[split:]
  predictions = list()
  xtrain = np.reshape(train.index, (-1, 1))
  xtest = np.reshape(test.index, (-1, 1))
  for t in range(len(test)):
    bay_ridge = BayesianRidge()
    bay_ridge.fit(xtrain, train.values)
    output = bay_ridge.predict(xtest)
    predictions = pd.Series(output, index=test.index)
  brrRMSE.append(sqrt(mean_squared_error(test, output)))
  return predictions

brrRMSE = list()
predictedBRR = new_cases.apply(applyBRR, axis=1)

"""#### Support Vector Regression (SVR)"""

from sklearn.svm import SVR
def applySVR(data):
  split = int(len(data.dropna())*0.94)
  train = data.dropna()[:split]
  test = data.dropna()[split:]
  predictions = list()
  xtrain = np.reshape(train.index, (-1, 1))
  xtest = np.reshape(test.index, (-1, 1))
  for t in range(len(test)):
    svr = SVR()
    svr.fit(xtrain, train.values)
    output = svr.predict(xtest)
    predictions = pd.Series(output, index=test.index)
  svrRMSE.append(sqrt(mean_squared_error(test, output)))
  return predictions

svrRMSE = list()
predictedSVR = new_cases.apply(applySVR,axis=1)

"""#### Holt-Winterâ€™s Linear Trend"""

from statsmodels.tsa.holtwinters import Holt
def applyHolt(data):
  split = int(len(data.dropna())*0.94)
  train = data.dropna()[:split]
  test = data.dropna()[split:]
  p = Holt(train).fit()
  output = p.predict(start=test.index[0], end=test.index[-1])
  holtRMSE.append(sqrt(mean_squared_error(test, output)))
  return pd.Series(output, index=test.index)

holtRMSE = list()
predictedHolt = new_cases.apply(applyHolt, axis=1)

"""### Results

#### Comparison between different models
"""

fig, axs = plt.subplots(5,2, figsize=(30, 30))
for i in range(len(predictedPoly)):
  axs[i//2,i%2].plot(new_cases.iloc[i].dropna(), label="Actual")
  axs[i//2,i%2].plot(predictedPoly.iloc[i],label="Linear Regression")
  axs[i//2,i%2].plot(predictedARIMA.iloc[i],label="ARIMA")
  axs[i//2,i%2].plot(predictedBRR.iloc[i], label="BRR")
  axs[i//2,i%2].plot(predictedHolt.iloc[i], label="Holt")
  axs[i//2,i%2].plot(predictedSVR.iloc[i], label="SVR")
  axs[i//2,i%2].set(xlabel='days since first case was found', ylabel='new cases')
  axs[i//2,i%2].set_title(new_cases.index[i])
  axs[i//2,i%2].legend(loc='best')

"""#### Enlarged graph on actual vs predicted values"""

fig, axs = plt.subplots(5,2, figsize=(30, 30))
for i in range(len(predictedPoly)):
  tmp = new_cases.iloc[i].dropna()
  split = int(len(tmp)*0.94)
  axs[i//2,i%2].plot(tmp[split:], label="Actual")
  axs[i//2,i%2].plot(predictedPoly.iloc[i], label="Linear Regression")
  axs[i//2,i%2].plot(predictedARIMA.iloc[i], label="ARIMA")
  axs[i//2,i%2].plot(predictedBRR.iloc[i], label="BRR")
  axs[i//2,i%2].plot(predictedHolt.iloc[i], label="Holt")
  axs[i//2,i%2].plot(predictedSVR.iloc[i], label="SVR")
  axs[i//2,i%2].set(xlabel='days since first case was found', ylabel='new cases')
  axs[i//2,i%2].set_title(new_cases.index[i])
  axs[i//2,i%2].legend(loc='best')

RMSE = pd.DataFrame({"Linear Regression": polyRMSE, "ARIMA": arimaRMSE, 
                     "BRR": brrRMSE, "SVR": svrRMSE, 
                     "Holt": holtRMSE}, index=new_cases.index)
RMSE

fig, axes = plt.subplots(5,2, figsize=(30, 30))
for i in range(len(predictedPoly)):
  colormat=np.where(RMSE.iloc[i] == RMSE.iloc[i].min(), 'g','b')
  RMSE.iloc[i].plot(ax=axes[i//2,i%2], kind='bar', color=colormat)
  axes[i//2,i%2].set(xlabel="Model", ylabel="RMSE value")
  axes[i//2,i%2].set_title(new_cases.index[i])